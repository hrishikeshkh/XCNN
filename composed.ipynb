{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST XCNN with graident ascent and feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, Conv2D, Flatten, Dense\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess the MNIST dataset. (build a binary classifier for 8 and not 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Modify labels: 8s are labeled as 1, non-8s are labeled as 0\n",
    "y_train_binary = np.where(y_train == 8, 1, 0)\n",
    "y_test_binary = np.where(y_test == 8, 1, 0)\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train_binary, y_val_binary = train_test_split(X_train, y_train_binary, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a simple CNN ending with a 1x1x50 convolution passed to a FC layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 2)         20        \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 28, 28, 50)        150       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 1, 1, 50)          1960050   \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1960271 (7.48 MB)\n",
      "Trainable params: 1960271 (7.48 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the input layer\n",
    "input_layer = Input(shape=(28, 28, 1))\n",
    "\n",
    "# Convolutional layers\n",
    "conv1 = Conv2D(2, (3, 3), padding='same', activation='relu')(input_layer)\n",
    "conv2 = Conv2D(50, (1, 1), activation='relu')(conv1)\n",
    "conv3 = Conv2D(50, (28, 28), activation='relu')(conv2)\n",
    "\n",
    "# Flatten the output for the fully connected layer\n",
    "flatten = Flatten()(conv3)\n",
    "\n",
    "# Fully connected and output layers\n",
    "output_layer = Dense(1, activation='sigmoid')(flatten)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Load the pre-trained model or train it\n",
    "try:\n",
    "    model = tf.keras.models.load_model('model/xcnn.h5')\n",
    "except:\n",
    "    history = model.fit(X_train, y_train_binary, epochs=5, batch_size=64, validation_data=(X_val, y_val_binary))\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test_binary)\n",
    "    print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save('model/xcnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_2\n",
      "conv2d_5\n",
      "conv2d_6\n",
      "conv2d_7\n",
      "conv2d_8\n",
      "conv2d_9\n",
      "flatten_1\n",
      "dense_1\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submodel(model, layer_name):\n",
    "  return tf.keras.models.Model(\n",
    "      model.input,\n",
    "      model.get_layer(layer_name).output\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image():\n",
    "  return tf.random.uniform((28,28, 1), minval = -0.5, maxval = 0.5)\n",
    "\n",
    "#plots the image\n",
    "def plot_image(image, title = 'random'):\n",
    "  image = image - tf.math.reduce_min(image)\n",
    "  image = image / tf.math.reduce_max(image)\n",
    "  plt.imshow(image)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.title(title)\n",
    "  plt.show()\n",
    "\n",
    "#gradient ascent for maximising the given filter's output\n",
    "def visualise_filters(model, layer_name, f_index = None, iters = 2000):\n",
    "  submodel = get_submodel(model, layer_name)\n",
    "  num_filters = submodel.output.shape[-1]\n",
    "\n",
    "  if f_index is None:\n",
    "    f_index = random.randint(0, num_filters - 1)\n",
    "\n",
    "  assert num_filters > f_index, 'filter index out of bounds'\n",
    "\n",
    "  image = create_image()\n",
    "  verbose_step = int(iters / 100)\n",
    "\n",
    "  for i in range(0, iters):\n",
    "    with tf.GradientTape() as tape:\n",
    "      tape.watch(image)\n",
    "      out = submodel(tf.expand_dims(image, axis = 0))[:, :, :, f_index]\n",
    "      loss = tf.math.reduce_mean(out)\n",
    "    grads = tape.gradient(loss, image)\n",
    "    grads = tf.math.l2_normalize(grads)\n",
    "    image += grads * 10\n",
    "\n",
    "  \n",
    "  if (i + 1) % verbose_step == 0:\n",
    "      print(f'Iteration: {i + 1}, Loss: {loss.numpy():.4f}')\n",
    "\n",
    "  plot_image(image, f'{layer_name}, {f_index}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform activation mapping for a particular image (using a positive class example for starters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that the given image is 8 : 0.7153711318969727\n"
     ]
    }
   ],
   "source": [
    "image = X_test[61]\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(image.reshape(28, 28), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Make the model predict on the image\n",
    "prediction = model.predict(image.reshape(1, 28, 28, 1), verbose = 0)\n",
    "print(f'Probability that the given image is 8 : {prediction[0][0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#getting the weights of the last layer\n",
    "weights_FC = model.layers[-1].get_weights()[0]\n",
    "\n",
    "\n",
    "#getting the output of the last convolutional layer\n",
    "model_penultimate = get_submodel(model, 'flatten_1')\n",
    "\n",
    "#obtain the output of the last convolutional layer\n",
    "activations_penultimate = model_penultimate.predict(image.reshape(1, 28, 28, 1), verbose = 0)\n",
    "\n",
    "print(activations_penultimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#defning a threshold value, if it's greater then the corresponding filter must be bright for the output\n",
    "threshold = 0.5\n",
    "\n",
    "#obtaining the dot product of the weights and the activations\n",
    "dot_product = np.multiply(weights_FC[0], activations_penultimate[0])\n",
    "\n",
    "#store the indices of the filters that are bright\n",
    "indices = np.where(dot_product > threshold)[0]\n",
    "\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over the indices and gradient ascent to obtain the images that maximize the output of the filters\n",
    "for i in indices:\n",
    "    visualise_filters(model = model, layer_name = 'conv2d_30' , f_index = i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
