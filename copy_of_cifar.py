# -*- coding: utf-8 -*-
"""Copy of Cifar

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fpi0VwquyMekCyfadzd01K1ap1seCUq8
"""

import tensorflow as tf
import numpy as np
from keras import datasets, layers, models
import matplotlib.pyplot as plt
from tensorflow.keras.models import load_model

# Load CIFAR-10 dataset
(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

# Define the selected classes
selected_classes = ['airplane', 'automobile', 'bird']
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

selected_indices = [class_names.index(class_name) for class_name in selected_classes]
#selected_indices = [class_names.index(class_name) for class_name in class_names]

# Filter the dataset to include only the selected classes
train_mask = [label[0] in selected_indices for label in train_labels]
test_mask = [label[0] in selected_indices for label in test_labels]

train_images = train_images[train_mask]
train_labels = train_labels[train_mask]

test_images = test_images[test_mask]
test_labels = test_labels[test_mask]

# Normalize pixel values to be between 0 and 1
train_images, test_images = train_images / 255.0, test_images / 255.0

!pip install tensorflow==2.15.0
!pip install keras==2.15.0

model = load_model('/content/cifar_cnn_gap.h5')

model.summary()

img = test_images[61]

# Plot the image
plt.imshow(img)
plt.axis('off')
plt.show()

# Make the model predict on the image
prediction = model.predict(img.reshape(1, 32, 32, 3), verbose=0)
print(f'Probability that the given image is 8 : {prediction[0][0]}')

def get_submodel(model, layer_name):
  return tf.keras.models.Model(
      model.input,
      model.get_layer(layer_name).output
  )

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from keras import datasets, layers, models
import keras

import textwrap
import cv2
import json
from keras.models import Model
# import PIL.Image





def make_gradcam_heatmap(img_array, model, last_conv_layer_name):

    grad_model = tf.keras.Model(
        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.get_layer('global_average_pooling2d_2').output]
    )


    with tf.GradientTape() as tape:
        last_conv_layer_output, preds = grad_model(img_array)
        num_neurons = preds.shape[1]


    heatmaps = []
    for class_index in range(num_neurons):

        with tf.GradientTape() as tape:
            last_conv_layer_output, preds = grad_model(img_array)
            class_channel = preds[:, class_index]


        grads = tape.gradient(class_channel, last_conv_layer_output)

        if grads is not None:
            pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

            heatmap = tf.reduce_sum(last_conv_layer_output[0] * pooled_grads, axis=-1)
            heatmaps.append(heatmap)
        else:
            print(f"No gradients for class {class_index}")
            heatmaps.append(tf.zeros_like(last_conv_layer_output[0][:, :, 0]))

    return heatmaps


def plot_heatmaps_with_titles(heatmaps, size=(128,128), threshold_factor=0.5, boundary_color=(0, 0, 255)):
    num_heatmaps = len(heatmaps)
    num_cols = min(num_heatmaps, 4)
    num_rows = (num_heatmaps + num_cols - 1) // num_cols  # Calculate the number of rows needed

    fig, axs = plt.subplots(num_rows, num_cols, figsize=(4*num_cols, 4*num_rows))

    for i, heatmap in enumerate(heatmaps):
        if not isinstance(heatmap, np.ndarray):
            heatmap = np.array(heatmap)  # Convert to NumPy array if not already one
        resized_heatmap = cv2.resize(heatmap, dsize=size, interpolation=cv2.INTER_CUBIC)

        # # Normalize the heatmap to [0, 1]
        # resized_heatmap = (resized_heatmap - np.min(resized_heatmap)) / (np.max(resized_heatmap) - np.min(resized_heatmap))

        threshold = 0.2 * np.max(resized_heatmap)
        mask = np.where(resized_heatmap > threshold, 1, 0)

        # Find contours of the mask
        contours, _ = cv2.findContours(np.uint8(mask), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        for contour in contours:
            cv2.drawContours(resized_heatmap, [contour], -1, boundary_color, 1)

        ax = axs[i // num_cols, i % num_cols]
        ax.imshow(resized_heatmap, cmap='gray')
        ax.set_title(f'Neuron {i + 1}')
        ax.axis('off')

    for j in range(i + 1, num_rows * num_cols):
        ax = axs[j // num_cols, j % num_cols]
        ax.axis('off')

    plt.tight_layout()
    plt.show()


def save_heatmap_and_image(heatmap, original_image, save_path):
    plt.figure(figsize=(10, 5))
    heatmap = np.array(heatmap)
    original_image = np.array(original_image)

    # Plot original image
    print(original_image)

    plt.subplot(1, 2, 1)
    plt.imshow(original_image)
    plt.axis('off')

    # Plot heatmap
    heatmap_resized = cv2.resize(heatmap, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)
    plt.subplot(1, 2, 2)
    plt.imshow(heatmap_resized, cmap='gray')
    plt.axis('off')

    plt.savefig(save_path)
    plt.close()


def analyze_neuron_influences_with_images(model, layer_name, img_array,image_org, threshold_pos=0.001, threshold_neg=-0.001, num_buckets=10):
    # Get the layer and weights
    layer = model.get_layer(layer_name)
    weights = layer.get_weights()[0]  # Assuming weights are at index 0
    print(f"Weight shape: {weights.shape}")

    # Initialize influence dictionaries with image data and CAM placeholders
    positive_influences, negative_influences, neutral_influences = [], [], []

    # Analyze weights for each neuron
    for neuron_index in range(weights.shape[3]):
        neuron_weights = weights[:, :, :, neuron_index]

        # Calculate mean and standard deviation
        mean_weight = np.mean(neuron_weights)
        std_dev = np.std(neuron_weights)

        # Classify influence based on thresholds
        influence_category = None
        if mean_weight > threshold_pos:
            influence_category = "positive"
        elif mean_weight < threshold_neg:
            influence_category = "negative"
        else:
            influence_category = "neutral"

        # Create influence entry with layer name, neuron number, image data, and CAM placeholder
        influence_entry = {
            "layer_name": layer_name,
            "neuron_number": neuron_index,
            "image_path": None,
            "cam_path": None,
            "mean_weight": mean_weight,
        }

        # Append influence entry to the appropriate list
        if influence_category:
            # Pre-process image for the model (replace with your pre-processing steps)
            original_image = image_org

            influence_entry["image_path"] = f"image_neuron_{layer_name}_{neuron_index}.png"  # Assign image path

            heatmaps = make_gradcam_heatmap(img_array, model, layer_name)
            heatmap = heatmaps[neuron_index]
            heatmap_path = f"assets/heatmap_neuron_{layer_name}_{neuron_index}.png"
            save_heatmap_and_image(heatmap, original_image, heatmap_path)  # Save heatmap using matplotlib
            influence_entry["heatmap_path"] = heatmap_path  # Assign heatmap path

            if influence_category == "positive":
                positive_influences.append(influence_entry)
            elif influence_category == "negative":
                negative_influences.append(influence_entry)
            else:
                neutral_influences.append(influence_entry)

    # Save results to JSON file
    result = {
        "positive": positive_influences,
        "negative": negative_influences,
        "neutral": neutral_influences,
    }

    with open('neuron_influence_results.json', 'w') as json_file:
        json.dump(result, json_file, indent=5, default=lambda x: x.tolist() if isinstance(x, (np.ndarray, np.float32)) else x)
    return True

def visualize_explanations(model, image, layer_name, mode="grayscale"):
    heatmaps = make_gradcam_heatmap(image, model, layer_name)
    return heatmaps

layer_names = [layer.name for layer in model.layers]

layer_names

layer_name = "conv2d_6"

image = np.expand_dims(img, axis=0)

heatmaps = visualize_explanations(model, image, layer_name)

plot_heatmaps_with_titles(heatmaps)

Vis_prmpt = """Imagine you're evaluating a Computer Vision model designed to identify objects, particularly airplanes, in images. You're provided with an image and a corresponding heatmap, which highlights areas the model focused on when making its prediction. The brighter or whiter areas in the heatmap indicate stronger activations, meaning those regions were more influential in the model's decision. Your task is to analyze both the image and the heatmap to explain, in simple terms, why the model classified the image as containing an airplane. Focus on the prominent features or patterns highlighted by the heatmap and describe how they likely contributed to the model's prediction. Explain your observations as if you are explaining to a person without a technical background."""

Human_Prompt = f"""
    Goal:

    Your objective is to conduct a thorough analysis of these inferences and their associated weights to provide a precise and comprehensive explanation of the CNN's prediction. Utilize the provided inferences to derive reasoning without making assumptions.

    Presentation:

    Present your analysis in a clear and understandable manner, as if explaining to a layperson. Pay close attention to the given inferences and ensure clarity in your explanations.

    Output Format:

    Influence Category:
    Mean Weight:
    Region of Lung Captured By this Category:
    Mention the features in this region indicative of pneumonia.
    Inference:
    Overall Analysis:

    Explain in simple terms why do you feel model classfied it as pneumonia. Provide strong reasoning. Avoid assuming additional information.

    Given:
    - The inference of neurons having a positive influence on prediction, along with its mean weight: {s1}
    - The inference of neurons having a negative influence on prediction, along with its mean weight: {s2}
    - The inference of neurons having a neutral influence on prediction, along with its mean weight: {s3}

            """

def analyze_neuron_influences_with_images(model, layer_name, img_array,image_org, threshold_pos=0.001, threshold_neg=-0.001, num_buckets=10):
    # Get the layer and weights
    layer = model.get_layer(layer_name)
    weights = layer.get_weights()[0]  # Assuming weights are at index 0
    print(f"Weight shape: {weights.shape}")

    # Initialize influence dictionaries with image data and CAM placeholders
    positive_influences, negative_influences, neutral_influences = [], [], []

    # Analyze weights for each neuron
    for neuron_index in range(weights.shape[3]):
        neuron_weights = weights[:, :, :, neuron_index]

        # Calculate mean and standard deviation
        mean_weight = np.mean(neuron_weights)
        std_dev = np.std(neuron_weights)

        # Classify influence based on thresholds
        influence_category = None
        if mean_weight > threshold_pos:
            influence_category = "positive"
        elif mean_weight < threshold_neg:
            influence_category = "negative"
        else:
            influence_category = "neutral"

        # Create influence entry with layer name, neuron number, image data, and CAM placeholder
        influence_entry = {
            "layer_name": layer_name,
            "neuron_number": neuron_index,
            "image_path": None,
            "cam_path": None,
            "mean_weight": mean_weight,
        }

        # Append influence entry to the appropriate list
        if influence_category:
            # Pre-process image for the model (replace with your pre-processing steps)
            original_image = image_org

            influence_entry["image_path"] = f"image_neuron_{layer_name}_{neuron_index}.png"  # Assign image path

            heatmaps = make_gradcam_heatmap(img_array, model, layer_name)
            heatmap = heatmaps[neuron_index]
            heatmap_path = f"/content/heatmap_neuron_{layer_name}_{neuron_index}.png"
            save_heatmap_and_image(heatmap, original_image, heatmap_path)  # Save heatmap using matplotlib
            influence_entry["heatmap_path"] = heatmap_path  # Assign heatmap path

            if influence_category == "positive":
                positive_influences.append(influence_entry)
            elif influence_category == "negative":
                negative_influences.append(influence_entry)
            else:
                neutral_influences.append(influence_entry)

    # Save results to JSON file
    result = {
        "positive": positive_influences,
        "negative": negative_influences,
        "neutral": neutral_influences,
    }

    with open('neuron_influence_results.json', 'w') as json_file:
        json.dump(result, json_file, indent=5, default=lambda x: x.tolist() if isinstance(x, (np.ndarray, np.float32)) else x)
    return True

analyze_neuron_influences_with_images(model, layer_name, image , img)

def extract_info_from_json(json_file):
    with open(json_file, 'r') as f:
        data = json.load(f)

    heatmap_paths = {
        "positive": [neuron["heatmap_path"] for neuron in data["positive"]],
        "negative": [neuron["heatmap_path"] for neuron in data["negative"]],
        "neutral": [neuron["heatmap_path"] for neuron in data["neutral"]],
    }

    mean_weights = {
        "positive": [neuron["mean_weight"] for neuron in data["positive"]],
        "negative": [neuron["mean_weight"] for neuron in data["negative"]],
        "neutral": [neuron["mean_weight"] for neuron in data["neutral"]],
    }

    return heatmap_paths, mean_weights

PROMPT = """ As an AI assistant, you will analyze images from the CIFAR dataset, focusing on those classified
as containing an airplane. Additionally, you will receive a heatmap highlighting feature activations from a CNN model
that classified the image as containing an airplane. Carefully examine the image and the grayscale heatmap to
 explain the reasoning behind the CNN's prediction. The brighter or whiter areas in the heatmap indicate stronger activations, meaning those regions were more influential in the model's decision. Explain your observations as if you are
   explaining to a person without a technical background, focusing on the prominent features or patterns highlighted by the heatmap and describe how they likely contributed to the model's prediction.
"""

!pip install -U --quiet langchain-google-genai  langchain

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate

from google.colab import userdata
import os
os.environ["GOOGLE_API_KEY"] = userdata.get('Gem_API')

!pip install pillow

import PIL.Image

def vision_model(img):
    model = genai.GenerativeModel('gemini-pro-vision')


    img = PIL.Image.open(img)

    res = model.generate_content([
    PROMPT,img] , stream = False,
        generation_config=genai.types.GenerationConfig( temperature=0.1))
    res.resolve()
    return res.text

s1 , s2, s3 = "" , ""  ,""

Human_Prompt = f"""
    Goal:

    Your objective is to conduct a thorough analysis of these inferences and their associated weights to provide a precise and comprehensive explanation of the CNN's prediction. Utilize the provided inferences to derive reasoning without making assumptions.

    Presentation:

    Present your analysis in a clear and understandable manner, as if explaining to a layperson. Pay close attention to the given inferences and ensure clarity in your explanations.

    Output Format:

    Influence Category:
    Mean Weight:
    Region of Image Captured By this Category:  # Changed from "Region of Lung"
    Mention the features in this region indicative of an airplane.  # Changed from "indicative of pneumonia"
    Inference:
    Overall Analysis:

    Explain in simple terms why do you feel the model classified it as an airplane. Provide strong reasoning. Avoid assuming additional information.  # Changed from "classified it as pneumonia"

    Given:
    - The inference of neurons having a positive influence on prediction, along with its mean weight: {s1}
    - The inference of neurons having a negative influence on prediction, along with its mean weight: {s2}
    - The inference of neurons having a neutral influence on prediction, along with its mean weight: {s3}

            """

!pip install -q google-generativeai

import google.generativeai as genai
import textwrap

def prefetch():
    heatmap_paths, mean_weights = extract_info_from_json("neuron_influence_results.json")
    pos = vision_model( heatmap_paths["positive"][0]  )
    pos = f"{pos} \n The weight for this inference is {mean_weights['positive'][0]}"
    neg = vision_model( heatmap_paths["negative"][0] )
    neg = f"{neg} \n The weight for this inference is {mean_weights['negative'][0]}"
    neu = vision_model(heatmap_paths["neutral"][0] )
    neu = f"{neu} \n The weight for this inference is {mean_weights['neutral'][0]}"
    print("Positive" , pos)
    print("\n Negative" , neg)
    print("\n Neutral" , neu)
    return pos , neg , neu


def text_model(hprompt ):
    gemini_llm = ChatGoogleGenerativeAI(model="gemini-pro",temperature=0.2)
    prompt = PromptTemplate(input_variables=[  "s1" , "s2" , "s3" ], template = hprompt)
    chain = prompt | gemini_llm
    pos , neg , neu = prefetch()
    res = chain.invoke({"s1": pos , "s2": neg , "s3" : neu})


    return res

res = text_model(Human_Prompt)